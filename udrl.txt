Standard RL predicts rewards, while RL instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. RL learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. RL generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time!